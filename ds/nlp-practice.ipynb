{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d319163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files\n",
    "import os\n",
    "\n",
    "# for punc list\n",
    "import string\n",
    "\n",
    "# for regex \n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffd776",
   "metadata": {},
   "source": [
    "Contrived NLP example:\n",
    "\n",
    "* ~read in raw text files from a directory with other stuff in it~ \n",
    "    * ~jane austen's books~\n",
    "* tokenize\n",
    "    * words\n",
    "    * sentences\n",
    "* clean\n",
    "    * remove punctuation\n",
    "    * remove numbers\n",
    "    * remove stopwords (from pre-baked list)\n",
    "    * use regex for something\n",
    "* create features\n",
    "    * unigrams\n",
    "    * bigrams\n",
    "    * lexicon-based\n",
    "* pass through classification model\n",
    "* calculating important metrics\n",
    "    * most frequent words\n",
    "    * tf-idf\n",
    "    * most mentioned entities\n",
    "    * simple sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efc64737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in text files\n",
    "def read_austen_data(directory_name):\n",
    "    texts = []\n",
    "    files = [f for f in listdir(directory_name) if isfile(join(directory_name, f))]\n",
    "    \n",
    "    for file in files:\n",
    "        if (\"austen\" in file):\n",
    "            with open(directory_name + file, encoding=\"utf-8\") as f:\n",
    "                #lowercase text and append\n",
    "                texts.append(f.read().lower())\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42d89ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "austen_books = read_austen_data(\"data/\")\n",
    "\n",
    "print(len(austen_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f098d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_tokenize(text):\n",
    "    \n",
    "    # replace new lines\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "        \n",
    "    # we need to strip all the extraneous spaces (more than 2)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    \n",
    "    # listify, splitting on spaces\n",
    "    text = text.split(' ')\n",
    "    \n",
    "    # replace all punctuation\n",
    "    text = [word for word in text if word not in string.punctuation]\n",
    "    \n",
    "    # the text starts as soon as we find \"chapter 1\" or \"chapter i\"\n",
    "    # so let's move the book to just its relevant parts by finding and deleting content before chapter 1|i\n",
    "    for i in range(len(text) - 1):\n",
    "        window_val = ' '.join(text[i:i+2])\n",
    "        if (window_val == \"chapter 1\" or window_val == \"chapter i\"):\n",
    "            print(i)\n",
    "            \n",
    "    ## now let's remove chapter numbers\n",
    "    \n",
    "    ## and remove volume numbers\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ead0639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373\n",
      "39229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['northanger',\n",
       " 'abbey',\n",
       " 'biographical',\n",
       " 'notice',\n",
       " 'of',\n",
       " 'the',\n",
       " 'author',\n",
       " '1',\n",
       " 'the',\n",
       " 'following']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_tokenize(austen_books[0])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3a121c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello timboschtt\n"
     ]
    }
   ],
   "source": [
    "tim = \"hello timbo   schtt\"\n",
    "\n",
    "print(re.sub(\"\\s{2,}\", \"\", tim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d02bb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing/Cleaning practice\n",
    "\n",
    "## manually\n",
    "\n",
    "## remove new line chars\n",
    "\n",
    "## separate into words\n",
    "\n",
    "## remove punctuation\n",
    "\n",
    "## remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d35f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## w/ sklearn...\n",
    "\n",
    "## remove new line chars\n",
    "\n",
    "## separate into sentences\n",
    "\n",
    "## remove punctuation\n",
    "\n",
    "## remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a663a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning practice\n",
    "\n",
    "## stopwords (contrived, don't save)\n",
    "\n",
    "## regex to find and kill volume numbers (save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf7794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
