{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6057536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# for punc list\n",
    "import string\n",
    "\n",
    "# for regex \n",
    "import regex as re\n",
    "\n",
    "# for nltk (library) work\n",
    "import nltk\n",
    "\n",
    "# for sorting dict\n",
    "import operator\n",
    "\n",
    "# for collapsing experiments\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffd776",
   "metadata": {},
   "source": [
    "Contrived NLP example:\n",
    "\n",
    "* ~read in raw text files from a directory with other stuff in it~ \n",
    "    * ~jane austen's books~\n",
    "* tokenize\n",
    "    * ~words (manually)~\n",
    "    * ~words (nltk)~\n",
    "    * ~sentences (nltk)~\n",
    "* clean\n",
    "    * ~remove punctuation (manually)~\n",
    "    * ~remove numbers (nltk)~\n",
    "    * ~remove stopwords (from pre-baked list) (manually)~\n",
    "    * ~use regex for something (manually)~\n",
    "* calculating important metrics\n",
    "    * ~most frequent words~\n",
    "    * ~unigrams~\n",
    "    * ~bigrams~\n",
    "    * ~trigrams~\n",
    "    * tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9ee13",
   "metadata": {},
   "source": [
    "Practical Numpy example:\n",
    "\n",
    "* make a random array of 1,000 integers between 1 and 10,000\n",
    "* make an array with 17 0's\n",
    "* calculate five number summary\n",
    "* find the standard deviation and the mean\n",
    "* find the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc64737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in text files\n",
    "def read_austen_data(directory_name):\n",
    "    texts = []\n",
    "    files = [f for f in listdir(directory_name) if isfile(join(directory_name, f))]\n",
    "    \n",
    "    for file in files:\n",
    "        if (\"austen\" in file):\n",
    "            with open(directory_name + file, encoding=\"utf-8\") as f:\n",
    "                #lowercase text and append\n",
    "                texts.append(f.read().lower())\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4928009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "austen_books = read_austen_data(\"data/\")\n",
    "\n",
    "print(len(austen_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f266d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_tokenize(text):\n",
    "    \n",
    "    # replace new lines\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # special weird punc\n",
    "    text = text.replace(\"---\", \" \")\n",
    "\n",
    "    # remove punctuation here.\n",
    "    punc_list = [p for p in string.punctuation]\n",
    "    \n",
    "    text = ''.join([char for char in text if char not in punc_list])\n",
    "            \n",
    "    # we need to strip all the extraneous spaces (more than 2)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    \n",
    "    # remove volume numbers\n",
    "    text = re.sub(\"volume i{1,}|volume [0-9]{1,}|volume one|volume two|volume three\", \"\", text)\n",
    "    \n",
    "    # listify, splitting on spaces\n",
    "    text = text.split(' ')\n",
    "\n",
    "    # the text starts as soon as we find \"chapter 1\" or \"chapter i\"\n",
    "    # so let's move the book to just its relevant parts by finding and deleting content before the first chapter 1|i\n",
    "    # since the volume-paradigm has multiple ch1, ch2, and so on\n",
    "    # first, find chapter 1, and replace everything prior\n",
    "    \n",
    "    # then, convert back to string, replace all the chapters, then convert back to list\n",
    "    for i in range(len(text) - 1):\n",
    "        window_val = ' '.join(text[i:i+2])\n",
    "        if (window_val == \"chapter 1\" or window_val == \"chapter i\"):\n",
    "            text = text[i+2:]\n",
    "            break\n",
    "    \n",
    "    # back to string\n",
    "    text = ' '.join(text)\n",
    "        \n",
    "    # replace chapters\n",
    "    text = re.sub(\"chapter [a-z]+|chapter [0-9]+\", \"\", text)\n",
    "    \n",
    "    # back to list\n",
    "    text = text.split(\" \")\n",
    "    \n",
    "    # get rid of empties\n",
    "    text = [word for word in text if word != \"\"]\n",
    "    \n",
    "    ## method extensions (pseudocode ok)\n",
    "    ## removing stop words \n",
    "        # (word for word in book if word not in stopwords)\n",
    "    ## removing numbers \n",
    "        # (word for word in book if word not in [num_list])\n",
    "        # or you could use regex across the whole thing as a string then join it back\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a0a3bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively call the cleaning function\n",
    "austen_books_tokenized = [manual_tokenize(book) for book in austen_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0f809e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no', 'one', 'who', 'had', 'ever', 'seen', 'catherine', 'morland', 'in', 'her'], ['sir', 'walter', 'elliot', 'of', 'kellynchhall', 'in', 'somersetshire', 'was', 'a', 'man'], ['about', 'thirty', 'years', 'ago', 'miss', 'maria', 'ward', 'of', 'huntingdon', 'with'], ['emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home'], ['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man'], ['the', 'family', 'of', 'dashwood', 'had', 'been', 'long', 'settled', 'in', 'sussex'], ['emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home']]\n"
     ]
    }
   ],
   "source": [
    "# first ten words of austen's books\n",
    "print([book[:10] for book in austen_books_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8aef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## w/ ntlk...\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def tokenize_with_nltk(book, unit):\n",
    "        \n",
    "    if (unit == 'word'):\n",
    "        tokens=nltk.word_tokenize(book)\n",
    "    \n",
    "    ## dealing with new line chars can be a little annoying\n",
    "    elif (unit == 'sentence'): \n",
    "        tokens = []\n",
    "        paragraphs = [p for p in book.split('\\n') if p]\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            if (paragraph != ''):\n",
    "                tokens.append(sent_detector.tokenize(paragraph.strip()))\n",
    "        \n",
    "        tokens = sum(tokens, [])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835f5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['northanger abbey', 'biographical notice of the author', '1', 'the following pages are the production of a pen which has already contributed in no small degree to the entertainment of the public.', \"and when the public, which has not been insensible to the merits of 'sense and sensibility,' 'pride and prejudice,' 'mansfield park,' and 'emma,' shall be informed that the hand which guided that pen is now mouldering in the grave, perhaps a brief account of jane austen will be read with a kindlier sentiment than simple curiosity.\", 'short and easy will be the task of the mere biographer.', 'a life of usefulness, literature, and religion, was not by any means a life of event.', 'to those who lament their irreparable loss, it is consolatory to think that, as she never deserved disapprobation, so, in the circle of her family and friends, she never met reproof; that her wishes were not only reasonable, but gratified; and that to the little disappointments incidental to human life was never added, even for a moment, an abatement of good-will from any who knew her.', 'jane austen was born on the 16th of december, 1775, at steventon, in the county of hants.', 'her father was rector of that parish upwards of forty years.']\n"
     ]
    }
   ],
   "source": [
    "mansfield_park_nltk_sent_tokenized = tokenize_with_nltk(austen_books[0], 'sentence')\n",
    "\n",
    "## first ten \"sentences\" of Mansfield park\n",
    "## note that we have a harder time really \"cleaning\" up this text\n",
    "## for large enough corpora of course, this does not matter.\n",
    "print(mansfield_park_nltk_sent_tokenized[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cd9ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'present', 'age', 'it', 'is', 'hazardous', 'to', 'mention', 'accomplishments', '.', 'our', 'authoress', 'would', ',', 'probably', ',', 'have', 'been', 'inferior', 'to', 'few', 'in', 'such', 'acquirements', ',', 'had', 'she', 'not', 'been', 'so', 'superior', 'to', 'most', 'in', 'higher', 'things', '.', 'she', 'had', 'not', 'only', 'an', 'excellent', 'taste', 'for', 'drawing', ',', 'but', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "mansfield_park_nltk_word_tokenized = tokenize_with_nltk(austen_books[0], 'word')\n",
    "\n",
    "print(mansfield_park_nltk_word_tokenized[1000:1050])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c89097",
   "metadata": {},
   "source": [
    "nltk - remarks \n",
    "\n",
    "remove punctuation - if you really wanted to, you have to do it again manually\n",
    "\n",
    "alternatively nltk has an api that lets you use your own Regexes as the delimiters but this can lead to other issues\n",
    "\n",
    "also you need to be careful about the end result here because you might not intend for Don't to split apart into 2 words.\n",
    "    \n",
    "remove numbers - similar to above, but less side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87cae3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', 'however', 'little', 'known', 'the', 'feelings', 'or', 'views', 'of', 'such', 'a', 'man', 'may', 'be', 'on', 'his', 'first', 'entering', 'a', 'neighbourhood', 'this', 'truth', 'is', 'so', 'well', 'fixed', 'in', 'the', 'minds', 'of', 'the', 'surrounding', 'families', 'that', 'he', 'is', 'considered', 'the', 'rightful', 'property', 'of', 'some', 'one', 'or', 'other', 'of', 'their', 'daughters', 'my', 'dear', 'mr', 'bennet', 'said', 'his', 'lady', 'to', 'him', 'one', 'day', 'have', 'you', 'heard', 'that', 'netherfield', 'park', 'is', 'let', 'at', 'last', 'mr', 'bennet', 'replied', 'that', 'he', 'had', 'not', 'but', 'it']\n",
      "{'it': 1520, 'is': 858, 'a': 1941, 'truth': 27, 'universally': 3, 'acknowledged': 20, 'that': 1566, 'single': 12, 'man': 142, 'in': 1861}\n",
      "['the: 4321', 'to: 4127', 'of: 3596', 'and: 3528', 'her: 2215', 'i: 2050', 'a: 1941', 'in: 1861', 'was: 1843', 'she: 1703']\n",
      "False\n",
      "it is\n"
     ]
    }
   ],
   "source": [
    "## Text metrics\n",
    "\n",
    "## unigrams (tokens)\n",
    "pride = austen_books_tokenized[4]\n",
    "\n",
    "print(pride[:100])\n",
    "\n",
    "pride_unigrams = list(set(pride))\n",
    "\n",
    "## word frequencies, then return top ten\n",
    "counts = {}\n",
    "for word in pride:\n",
    "    if (word not in counts):\n",
    "        counts[word] = 1\n",
    "    else:\n",
    "        counts[word] += 1\n",
    "        \n",
    "## unsorted output:\n",
    "print({k: counts[k] for k in list(counts)[:10]})\n",
    "\n",
    "## now sort the dictionary by value\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "## use list comprehension to draw top 10 most frequent words\n",
    "print([word + \": \" + str(freq) for word, freq in sorted_counts[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8500fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of the: 463', 'to be: 442', 'in the: 382', 'i am: 301', 'of her: 260', 'to the: 251', 'it was: 250', 'mr darcy: 241', 'of his: 235', 'she was: 212']\n"
     ]
    }
   ],
   "source": [
    "pride = austen_books_tokenized[4]\n",
    "\n",
    "## most frequent bigrams\n",
    "bigrams = {}\n",
    "## decrement range by 2 to avoid going out of bounds\n",
    "for i in range(len(pride) - 2):\n",
    "    bigram = ' '.join(pride[i:i+2])\n",
    "    if (bigram not in bigrams):\n",
    "        bigrams[bigram] = 1\n",
    "    else:\n",
    "        bigrams[bigram] += 1\n",
    "        \n",
    "## sorted call: (dictionary items, key=operator.itemgetter(1), reverse = True)\n",
    "sorted_bigrams = sorted(bigrams.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "print([word + \": \" + str(val) for word, val in sorted_bigrams[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b135eeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i do not: 61\n",
      "i am sure: 61\n",
      "as soon as: 55\n",
      "she could not: 49\n",
      "that he had: 37\n",
      "in the world: 34\n",
      "it would be: 33\n",
      "i am not: 32\n",
      "i dare say: 31\n",
      "it was not: 30\n",
      "could not be: 30\n",
      "that he was: 29\n",
      "that it was: 28\n",
      "on the subject: 28\n",
      "would have been: 27\n",
      "as well as: 27\n",
      "by no means: 26\n",
      "and she was: 25\n",
      "one of the: 25\n",
      "he had been: 25\n",
      "that she had: 24\n",
      "the rest of: 23\n",
      "i did not: 23\n",
      "a great deal: 23\n",
      "in spite of: 23\n",
      "it was a: 23\n",
      "do not know: 22\n",
      "i have not: 22\n",
      "uncle and aunt: 22\n",
      "she did not: 22\n",
      "mrs bennet was: 21\n",
      "not to be: 21\n"
     ]
    }
   ],
   "source": [
    "pride = austen_books_tokenized[4]\n",
    "## trigrams!\n",
    "trigrams = {}\n",
    "\n",
    "for i in range(len(pride) - 3):\n",
    "    trigram = ' '.join(pride[i: i+3])\n",
    "    if (trigram not in trigrams):\n",
    "        trigrams[trigram] = 1\n",
    "    else:\n",
    "        trigrams[trigram] += 1\n",
    "\n",
    "## remove any trigram that has a value of 1.\n",
    "# print(len(trigrams))\n",
    "trigrams = {word:freq for (word,freq) in trigrams.items() if freq > 20}\n",
    "# print(len(trigrams))\n",
    "\n",
    "## now do the sorting.\n",
    "sorted_trigrams = sorted(trigrams.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "print(\"\\n\".join([word + \": \" + str(val) for word, val in sorted_trigrams]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ab7be010",
   "metadata": {},
   "outputs": [],
   "source": [
    "## td-idf\n",
    "\n",
    "## where tf-df = frequency of word W in doc A / number of docs word A appears in\n",
    "\n",
    "## input / output ...\n",
    "## we need all the docs\n",
    "## check on a per doc level> for each doc> for each of its words ... \n",
    "def tf_idf(docs):\n",
    "    \n",
    "    uniques = list([list(set(doc)) for doc in docs])\n",
    "    \n",
    "    vocabulary = list(set(sum(uniques, [])))\n",
    "        \n",
    "    # arrange a matrix where each column is a word in the vocab and each row is a book\n",
    "    \n",
    "    # todo - do that.\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            \n",
    "    \n",
    "    return \"tim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "042b25ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tim'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf(austen_books_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "20217831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add', 'oh', 'absence', 'to'}\n"
     ]
    }
   ],
   "source": [
    "test1 = [\"absence\", \"oh\", \"absence\", \"add\", \"to\"]\n",
    "test2 = [\"absence\", \"add\", \"to\", \"to\", \"to\"]\n",
    "l = [test1, test2]\n",
    "\n",
    "print(set(list(itertools.chain.from_iterable(l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa4ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
