{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8f93282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files\n",
    "import os\n",
    "\n",
    "# for punc list\n",
    "import string\n",
    "\n",
    "# for regex \n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffd776",
   "metadata": {},
   "source": [
    "Contrived NLP example:\n",
    "\n",
    "* ~read in raw text files from a directory with other stuff in it~ \n",
    "    * ~jane austen's books~\n",
    "* tokenize\n",
    "    * words\n",
    "    * sentences\n",
    "* clean\n",
    "    * remove punctuation\n",
    "    * remove numbers\n",
    "    * remove stopwords (from pre-baked list)\n",
    "    * use regex for something\n",
    "* create features\n",
    "    * unigrams\n",
    "    * bigrams\n",
    "    * lexicon-based\n",
    "* pass through classification model\n",
    "* calculating important metrics\n",
    "    * most frequent words\n",
    "    * tf-idf\n",
    "    * most mentioned entities\n",
    "    * simple sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efc64737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in text files\n",
    "def read_austen_data(directory_name):\n",
    "    texts = []\n",
    "    files = [f for f in listdir(directory_name) if isfile(join(directory_name, f))]\n",
    "    \n",
    "    for file in files:\n",
    "        if (\"austen\" in file):\n",
    "            with open(directory_name + file, encoding=\"utf-8\") as f:\n",
    "                #lowercase text and append\n",
    "                texts.append(f.read().lower())\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e47bd789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "austen_books = read_austen_data(\"data/\")\n",
    "\n",
    "print(len(austen_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c078e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_tokenize(text):\n",
    "    \n",
    "    # replace new lines\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove punctuation here.\n",
    "    punc_list = [p for p in string.punctuation]\n",
    "    \n",
    "    text = ''.join([char for char in text if char not in punc_list])\n",
    "            \n",
    "    # we need to strip all the extraneous spaces (more than 2)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    \n",
    "    # remove volume numbers\n",
    "    text = re.sub(\"volume i{1,}|volume [0-9]{1,}|volume one|volume two|volume three\", \"\", text)\n",
    "    \n",
    "    # listify, splitting on spaces\n",
    "    text = text.split(' ')\n",
    "\n",
    "    # the text starts as soon as we find \"chapter 1\" or \"chapter i\"\n",
    "    # so let's move the book to just its relevant parts by finding and deleting content before the first chapter 1|i\n",
    "    # since the volume-paradigm has multiple ch1, ch2, and so on\n",
    "    # first, find chapter 1, and replace everything prior\n",
    "    \n",
    "    # then, convert back to string, replace all the chapters, then convert back to list\n",
    "    for i in range(len(text) - 1):\n",
    "        window_val = ' '.join(text[i:i+2])\n",
    "        if (window_val == \"chapter 1\" or window_val == \"chapter i\"):\n",
    "            text = text[i+2:]\n",
    "            break\n",
    "    \n",
    "    # back to string\n",
    "    text = ' '.join(text)\n",
    "        \n",
    "    # replace chapters\n",
    "    text = re.sub(\"chapter [a-z]+|chapter [0-9]+\", \"\", text)\n",
    "    \n",
    "    # back to list\n",
    "    text = text.split(\" \")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bd46eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively call the cleaning function\n",
    "austen_books_tokenized = [manual_tokenize(book) for book in austen_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fcf6ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no', 'one', 'who', 'had', 'ever', 'seen', 'catherine', 'morland', 'in', 'her'], ['sir', 'walter', 'elliot', 'of', 'kellynchhall', 'in', 'somersetshire', 'was', 'a', 'man'], ['about', 'thirty', 'years', 'ago', 'miss', 'maria', 'ward', 'of', 'huntingdon', 'with'], ['emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home'], ['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man'], ['the', 'family', 'of', 'dashwood', 'had', 'been', 'long', 'settled', 'in', 'sussex'], ['emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home']]\n"
     ]
    }
   ],
   "source": [
    "# first ten words of austen's books\n",
    "print([book[:10] for book in austen_books_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c58af615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing/Cleaning practice\n",
    "\n",
    "## manually\n",
    "\n",
    "## remove new line chars\n",
    "\n",
    "## separate into words\n",
    "\n",
    "## remove punctuation\n",
    "\n",
    "## remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## w/ sklearn...\n",
    "\n",
    "## remove new line chars\n",
    "\n",
    "## separate into sentences\n",
    "\n",
    "## remove punctuation\n",
    "\n",
    "## remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2d52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning practice\n",
    "\n",
    "## stopwords (contrived, don't save)\n",
    "\n",
    "## regex to find and kill volume numbers (save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4423c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
